{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "log_quality_anomaly_detection_20181108.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubermen/anomaly_detector/blob/master/log_quality_anomaly_detection_20181108.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0h_wqD9dVjyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf anomaly_detector\n",
        "!git clone https://github.com/ubermen/anomaly_detector.git\n",
        "%cd anomaly_detector\n",
        "!ls\n",
        "from preprocessor import Preprocessor\n",
        "from testor import TestUtil\n",
        "from vae_on_cnn import VariationalAutoencoder\n",
        "from latent_gen_2d import LatentGen2D\n",
        "\n",
        "import tensorflow as tf\n",
        "tfd = tf.contrib.distributions\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ryi76Jg3VmtV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "sequence_length = 20\n",
        "encoding_size = 128\n",
        "code_size = 2\n",
        "  \n",
        "preprocess = Preprocessor(sequence_length, encoding_size)\n",
        "test_data = preprocess.extract_from_bigquery('test', 'bi-service-155107', 'bigpi_test', 'log_anomaly_globalsignin_devicemodel_20181004_test_shuffled', print_result=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZt1DdrqVo2l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the model \n",
        "vae = VariationalAutoencoder(sequence_length, encoding_size, code_size, kernel=(sequence_length,10))\n",
        "data = tf.placeholder(tf.float32, [None, sequence_length, encoding_size])\n",
        "prior = vae.make_prior()\n",
        "posterior = vae.make_encoder(data)\n",
        "code = posterior.sample()\n",
        "\n",
        "# Define the loss.\n",
        "decoder = vae.make_decoder(code)\n",
        "likelihood = decoder.log_prob(data)\n",
        "divergence = tfd.kl_divergence(posterior, prior)\n",
        "elbo = tf.reduce_mean(likelihood - divergence)\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(-elbo)\n",
        "anomaly_score = -likelihood\n",
        "\n",
        "# checkpoint\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "test_util = TestUtil(anomaly_score, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kevU7AtHVqXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  save_path = saver.save(sess, checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPf2p8IGVr0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train\n",
        "total_epochs = 1\n",
        "extract_batch_size = 100000\n",
        "learning_batch_size = 1000\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  is_training = True\n",
        "  abnormal_samples = None\n",
        "  saver.restore(sess, checkpoint_dir)\n",
        "  for epoch in range(total_epochs):\n",
        "    s = (epoch*100)%len(test_data)\n",
        "    e = s + 100\n",
        "    test_util.test('e_{:04d}'.format(epoch), sess, test_data[s:e])\n",
        "    \n",
        "    i = 0\n",
        "    while True :\n",
        "      e_start = i\n",
        "      e_end = e_start + extract_batch_size - 1\n",
        "      i = i + extract_batch_size\n",
        "      train_batch_data = preprocess.extract_batch_from_bigquery('extract_batch', 'bi-service-155107', 'bigpi_test', 'log_anomaly_globalsignin_devicemodel_20181004_real', e_start, e_end)\n",
        "      data_count = train_batch_data.shape[0]\n",
        "      if data_count == 0 : break\n",
        "      j = 0\n",
        "      while j < data_count :\n",
        "        l_start = j\n",
        "        l_end = l_start + learning_batch_size - 1\n",
        "        j = j + learning_batch_size\n",
        "        feed = {data: train_batch_data[l_start:l_end]}\n",
        "        sess.run(optimize, feed)\n",
        "    print('finish epoch',epoch)\n",
        "    save_path = saver.save(sess, checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiX-WruWVs1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, checkpoint_dir)\n",
        "  test_util.test('test', sess, test_data[:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqSphOuPVuIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generation\n",
        "gen_count = 10000\n",
        "gen_codes = vae.make_decoder(prior.sample(gen_count)).mean()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, checkpoint_dir)\n",
        "  #test_util.test('generate', sess, samples)\n",
        "  result = {}\n",
        "  for i in range(1) :\n",
        "    samples = sess.run(gen_codes)\n",
        "    ascii_codes_list = test_util.convert_onehot_to_ascii(sess, samples)\n",
        "    print(ascii_codes_list)\n",
        "    score = test_util.get_score_list(sess, samples)\n",
        "    for j, ascii_code in enumerate(ascii_codes_list) :\n",
        "      result[ascii_code] = score[j]\n",
        "  sortable = []\n",
        "  for key in result:\n",
        "    value = result[key]\n",
        "    sortable.append((key,value))\n",
        "  sorted_by_value = sorted(sortable, key=lambda x: x[1])\n",
        "  print(sorted_by_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICHcixBlVvjr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generation from manually created codes\n",
        "manual_codes = tf.placeholder(tf.float32, [None, code_size])\n",
        "gen_codes_manual = vae.make_decoder(manual_codes).mean()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, checkpoint_dir)\n",
        "  #codes=[\\\n",
        "  #       [-1.5,-1.5], [-1.5,-1],[-1.5,-0.5],[-1.5,0],[-1.5,0.5],[-1.5,1],\n",
        "  #       [-1,-1.5],   [-1,-1],  [-1,-0.5],  [-1,0],  [-1,0.5],  [-1,1],\n",
        "  #       [-0.5,-1.5], [-0.5,-1],[-0.5,-0.5],[-0.5,0],[-0.5,0.5],[-0.5,1],\n",
        "  #       [0,-1.5],    [0,-1],   [0,-0.5],   [0,0],   [0,0.5],   [0,1],\n",
        "  #       [0.5,-1.5],  [0.5,-1], [0.5,-0.5], [0.5,0], [0.5,0.5], [0.5,1],\n",
        "  #       [1,-1.5],    [1,-1],   [1,-0.5],   [1,0],   [1,0.5],   [1,1],\n",
        "  #       [1.5,-1.5],  [1.5,-1], [1.5,-0.5], [1.5,0], [1.5,0.5], [1.5,1],\n",
        "  #       [2,-1.5],    [2,-1],   [2,-0.5],   [2,0],   [2,0.5],   [2,1],\n",
        "  #]\n",
        "  offset_array = [-3,-2,-1,0,1,2,3]\n",
        "  for i in offset_array :\n",
        "    print('------------------------------------------------')\n",
        "    for j in offset_array :\n",
        "      latent_gen = LatentGen2D(horizontal_offset=i, vertical_offset=j)\n",
        "      #latent_gen.print_codes()\n",
        "\n",
        "      #codes = sess.run(prior.sample(10))\n",
        "      samples = sess.run(gen_codes_manual, {manual_codes:latent_gen.codes})\n",
        "      ascii_codes_list = test_util.convert_onehot_to_ascii(sess, samples)\n",
        "      latent_gen.print_row_by_row(ascii_codes_list, latent_gen.width)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zN6xDirmV12D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generation from char sequences\n",
        "charseq = ['iPad6,4', 'iPhone','Samsung']\n",
        "onehot = preprocess.convert_str_array_to_onehot_ndarray('gen', charseq)\n",
        "manual_codes = tf.placeholder(tf.float32, [None, code_size])\n",
        "gen_codes_manual = vae.make_decoder(manual_codes).mean()\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, checkpoint_dir)\n",
        "  codes = sess.run([code], {data:onehot})[0]\n",
        "  samples = sess.run(gen_codes_manual, {manual_codes:codes})\n",
        "  test_util.test('generate', sess, samples)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}