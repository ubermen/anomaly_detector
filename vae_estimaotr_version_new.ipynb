{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ubermen/anomaly_detector/blob/master/vae_estimaotr_version_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sVwEbqedHUyB",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# This is only for when running on Colab:\n",
    "# Get the dependency .py files, if any.\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Authenticate the user for better GCS access.\n",
    "    # Copy verification code into the text field to continue.\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eb1-UOEqHVj5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# this code is modified version of tensorflow probability example\n",
    "# https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/vae.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "tfd = tf.contrib.distributions\n",
    "\n",
    "seq_len = 16\n",
    "enc_size = 128\n",
    "IMAGE_SHAPE = [seq_len, enc_size, 1]\n",
    "\n",
    "kernel_height = max(2, int(seq_len/2))\n",
    "kernel_width = max(2, int(enc_size/2))\n",
    "kernel = (kernel_height, kernel_width)\n",
    "\n",
    "stride_vertical = 1\n",
    "stride_horizontal = 2\n",
    "stride = (stride_vertical, stride_horizontal)\n",
    "\n",
    "flags.DEFINE_integer(\"max_steps\", default=1001, help=\"Number of training steps to run.\")\n",
    "flags.DEFINE_string(\"data_dir\", default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/data\"), help=\"Directory where data is stored (if using real data).\")\n",
    "flags.DEFINE_string(\"model_dir\", default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/\"), help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_integer(\"viz_steps\", default=100, help=\"Frequency at which to save visualizations.\")\n",
    "flags.DEFINE_integer(\"batch_size\", default=32, help=\"Batch size.\")\n",
    "flags.DEFINE_string(\"activation\", default=\"leaky_relu\", help=\"Activation function for all hidden layers.\")\n",
    "flags.DEFINE_string(\"encoder_id\", default=\"lqad_encoder\", help=\"\")\n",
    "flags.DEFINE_string(\"decoder_id\", default=\"lqad_decoder\", help=\"\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cEa317-eI_x6",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object) :\n",
    "  def __init__(self, sequence_length, encoding_size, code_size=2, kernel=None, stride=None, conv1_filter=16, conv2_filter=32) :\n",
    "    self.code_size = code_size\n",
    "    print('latent_size',self.code_size)\n",
    "    self.sequence_length = sequence_length\n",
    "    self.encoding_size = encoding_size\n",
    "    self.data_shape = [sequence_length, encoding_size]\n",
    "    self.is_training = True\n",
    "\n",
    "    if kernel is not None :\n",
    "      self.kernel = kernel\n",
    "    else :\n",
    "      kernel_height = max(2, int(self.sequence_length/2))\n",
    "      kernel_width = max(2, int(self.encoding_size/2))\n",
    "      self.kernel = (kernel_height, kernel_width)\n",
    "\n",
    "    print('kernel',self.kernel)\n",
    "\n",
    "    if stride is not None :\n",
    "      self.stride = stride\n",
    "    else :\n",
    "      stride_vertical = 1\n",
    "      stride_horizontal = 2\n",
    "      self.stride = (stride_vertical, stride_horizontal)\n",
    "\n",
    "    print('stride',self.stride)\n",
    "\n",
    "    self.conv1_filter = conv1_filter\n",
    "    self.conv2_filter = conv2_filter\n",
    "    self.conv_result_height = int(sequence_length / stride_vertical / stride_vertical)\n",
    "    self.conv_result_width = int(encoding_size / stride_horizontal / stride_horizontal)\n",
    "    self.final_conv_shape = [self.conv_result_height, self.conv_result_width, self.conv2_filter]\n",
    "    print('final_conv',self.final_conv_shape)\n",
    "    \n",
    "    self.make_encoder = tf.make_template(FLAGS.encoder_id, self.make_encoder)\n",
    "    self.make_decoder = tf.make_template(FLAGS.decoder_id, self.make_decoder)\n",
    "\n",
    "  def make_prior(self):\n",
    "    code_size = self.code_size\n",
    "    loc = tf.zeros(code_size)\n",
    "    scale = tf.ones(code_size)\n",
    "    return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "  def make_encoder(self, data):\n",
    "    code_size = self.code_size\n",
    "    sequence_length = self.sequence_length\n",
    "    encoding_size = self.encoding_size\n",
    "    conv1_filter = self.conv1_filter\n",
    "    conv2_filter = self.conv2_filter\n",
    "    kernel = self.kernel\n",
    "    stride = self.stride\n",
    "\n",
    "    # conv\n",
    "    x = tf.reshape(data, shape=[-1, sequence_length, encoding_size, 1])\n",
    "\n",
    "    conv1 = tf.layers.conv2d(x, conv1_filter, kernel, stride, activation=tf.nn.relu, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),bias_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv2 = tf.layers.conv2d(conv1, conv2_filter, kernel, stride, activation=tf.nn.relu, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),bias_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "    print('input',x)\n",
    "    print('output',conv1)\n",
    "    print('output',conv2)\n",
    "\n",
    "    # Flatten the data to a 1-D vector for the fully connected layer\n",
    "    x = tf.contrib.layers.flatten(conv2)\n",
    "    x = tf.layers.dense(x, encoding_size, tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    out = tf.layers.dense(x, code_size)\n",
    "    loc = tf.layers.dense(x, code_size)\n",
    "    scale = tf.layers.dense(out, code_size, tf.nn.softplus)\n",
    "    return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "  def make_decoder(self, code):\n",
    "    data_shape = self.data_shape\n",
    "    conv1_filter = self.conv1_filter\n",
    "    conv2_filter = self.conv2_filter\n",
    "    encoding_size = self.encoding_size\n",
    "    final_conv_shape = self.final_conv_shape\n",
    "    kernel = self.kernel\n",
    "    stride = self.stride\n",
    "\n",
    "    # deconv\n",
    "    x = code\n",
    "    x = tf.layers.dense(x, encoding_size, tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    x = tf.layers.dense(x, np.prod(final_conv_shape))\n",
    "    x = tf.reshape(x, shape=[-1] + final_conv_shape)\n",
    "\n",
    "    conv2 = tf.layers.conv2d_transpose(x, conv1_filter, kernel, stride, padding='same')\n",
    "    conv1 = tf.layers.conv2d_transpose(conv2, 1, kernel, stride, padding='same')\n",
    "\n",
    "    print('d_input',x)\n",
    "    print('d_output',conv2)\n",
    "    print('d_output',conv1)\n",
    "\n",
    "    logit = tf.reshape(conv1, [-1] + data_shape)\n",
    "    print('d_output',logit)\n",
    "    return tfd.Independent(tfd.Bernoulli(logit), 2)\n",
    "\n",
    "def model_fn(features, labels, mode, params, config):\n",
    "\n",
    "  # Define the model\n",
    "  vae = VariationalAutoencoder(seq_len, enc_size)\n",
    "\n",
    "  # preprocess data\n",
    "  data = preprocess(features)\n",
    "\n",
    "  # make prior\n",
    "  prior = vae.make_prior()\n",
    "\n",
    "  # make posterior and sample from data\n",
    "  posterior = vae.make_encoder(data)\n",
    "  code = posterior.sample()\n",
    "\n",
    "  # make decoder and likelihood from data\n",
    "  decoder = vae.make_decoder(code)\n",
    "  likelihood = decoder.log_prob(data)\n",
    "  distortion = -likelihood\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT :\n",
    "\n",
    "    # define prediction\n",
    "    prediction = {\n",
    "      '_0' : features,\n",
    "      '_1' : distortion\n",
    "    }\n",
    "\n",
    "    export_outputs = {\n",
    "      'prediction': tf.estimator.export.PredictOutput(prediction)\n",
    "    }\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "\n",
    "  else :\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Define the loss.\n",
    "    divergence = tfd.kl_divergence(posterior, prior)\n",
    "    elbo = tf.reduce_mean(likelihood - divergence)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    loss = -elbo\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    eval_metric_ops={\n",
    "      \"elbo\": tf.metrics.mean(elbo),\n",
    "      \"divergence\": tf.metrics.mean(divergence),\n",
    "      \"distortion\": tf.metrics.mean(distortion),\n",
    "    }\n",
    "\n",
    "    prediction = None\n",
    "    export_outputs = None\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    loss=loss,\n",
    "    train_op=train_op,\n",
    "    eval_metric_ops=eval_metric_ops,\n",
    "    predictions=prediction,\n",
    "    export_outputs=export_outputs,\n",
    "  )\n",
    "\n",
    "def static_nlog_dataset(data_dir, file_name):\n",
    "  dataset = tf.data.TextLineDataset(data_dir + '/' + file_name).skip(1)\n",
    "  return dataset\n",
    "\n",
    "def build_input_fns(data_dir, batch_size):\n",
    "  \"\"\"Builds an Iterator switching between train and heldout data.\"\"\"\n",
    "\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = static_nlog_dataset(data_dir, 'globalsignin_devicemodel_train')\n",
    "  training_dataset = training_dataset.shuffle(10000).repeat().batch(batch_size)\n",
    "  train_input_fn = lambda: training_dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "  # Build an iterator over the heldout set.\n",
    "  eval_dataset = static_nlog_dataset(data_dir, 'globalsignin_devicemodel_eval')\n",
    "  eval_dataset = eval_dataset.batch(batch_size)\n",
    "  eval_input_fn = lambda: eval_dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "  return train_input_fn, eval_input_fn\n",
    "\n",
    "def _get_session_config_from_env_var():\n",
    "  \"\"\"Returns a tf.ConfigProto instance that has appropriate device_filters set.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "\n",
    "  if (tf_config and 'task' in tf_config and 'type' in tf_config['task'] and\n",
    "          'index' in tf_config['task']):\n",
    "    # Master should only communicate with itself and ps\n",
    "    if tf_config['task']['type'] == 'master':\n",
    "      return tf.ConfigProto(device_filters=['/job:ps', '/job:master'])\n",
    "    # Worker should only communicate with itself and ps\n",
    "    elif tf_config['task']['type'] == 'worker':\n",
    "      return tf.ConfigProto(device_filters=[\n",
    "        '/job:ps',\n",
    "        '/job:worker/task:%d' % tf_config['task']['index']\n",
    "      ])\n",
    "  return None\n",
    "\n",
    "def serving_input_fn():\n",
    "  string_array = tf.placeholder(tf.string, [None])\n",
    "  return tf.estimator.export.TensorServingInputReceiver(string_array, string_array)\n",
    "\n",
    "def preprocess(string_array):\n",
    "  string_array = tf.strings.substr(string_array,0,seq_len)\n",
    "  split_stensor = tf.string_split(string_array, delimiter=\"\")\n",
    "  split_values = split_stensor.values\n",
    "  unicode_values = tf.map_fn(lambda x: tf.io.decode_raw(x, tf.uint8)[0], split_values, dtype=tf.uint8)\n",
    "  unicode_tensor = tf.sparse_to_dense(split_stensor.indices, [tf.shape(string_array)[0], seq_len], unicode_values, default_value=-1)\n",
    "  encoded_tensor = tf.map_fn(lambda x: tf.one_hot(x, enc_size), unicode_tensor, dtype=tf.float32)\n",
    "  return encoded_tensor\n",
    "\n",
    "def main(argv):\n",
    "  del argv  # unused\n",
    "\n",
    "  params = FLAGS.flag_values_dict()\n",
    "  params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
    "  tf.gfile.MakeDirs(FLAGS.model_dir)\n",
    "\n",
    "  train_input_fn, eval_input_fn = build_input_fns(FLAGS.data_dir, FLAGS.batch_size)\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "    train_input_fn, max_steps=FLAGS.max_steps)\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('exporter', serving_input_fn)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "    eval_input_fn,\n",
    "    steps=FLAGS.viz_steps,\n",
    "    exporters=[exporter],\n",
    "    name='lqad-eval')\n",
    "\n",
    "  run_config = tf.estimator.RunConfig(session_config=_get_session_config_from_env_var())\n",
    "  run_config = run_config.replace(model_dir=FLAGS.model_dir)\n",
    "\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn,\n",
    "    params=params,\n",
    "    config=run_config\n",
    "  )\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HQAIZ0lOHa44",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2520.0
    },
    "outputId": "34dfbdc8-2dd2-4469-ad2f-e329a636413c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'gs://bigus/colab_test_3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3d0534d160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "latent_size 2\n",
      "kernel (8, 64)\n",
      "stride (1, 2)\n",
      "final_conv [16, 32, 32]\n",
      "WARNING:tensorflow:From <ipython-input-3-c591ea0de0d1>:210: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-c591ea0de0d1>:42: MultivariateNormalDiag.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_diag) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/mvn_diag.py:224: MultivariateNormalLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_linear_operator) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:201: AffineLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.affine_linear_operator) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/affine_linear_operator.py:158: _DistributionShape.__init__ (from tensorflow.contrib.distributions.python.ops.shape) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "input Tensor(\"lqad_encoder/Reshape:0\", shape=(?, 16, 128, 1), dtype=float32)\n",
      "output Tensor(\"lqad_encoder/conv2d/Relu:0\", shape=(?, 16, 64, 16), dtype=float32)\n",
      "output Tensor(\"lqad_encoder/conv2d_1/Relu:0\", shape=(?, 16, 32, 32), dtype=float32)\n",
      "d_input Tensor(\"lqad_decoder/Reshape:0\", shape=(?, 16, 32, 32), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/conv2d_transpose/BiasAdd:0\", shape=(?, 16, 64, 16), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/conv2d_transpose_1/BiasAdd:0\", shape=(?, 16, 128, 1), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/Reshape_1:0\", shape=(?, 16, 128), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-3-c591ea0de0d1>:95: Independent.__init__ (from tensorflow.contrib.distributions.python.ops.independent) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/kullback_leibler.py:98: _kl_brute_force (from tensorflow.contrib.distributions.python.ops.mvn_linear_operator) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into gs://bigus/colab_test_3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1420.2129, step = 0\n",
      "INFO:tensorflow:global_step/sec: 3.40301\n",
      "INFO:tensorflow:loss = 67.58221, step = 100 (29.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.39416\n",
      "INFO:tensorflow:loss = 54.987297, step = 200 (29.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.39692\n",
      "INFO:tensorflow:loss = 54.485523, step = 300 (29.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.38531\n",
      "INFO:tensorflow:loss = 51.70526, step = 400 (30.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.25386\n",
      "INFO:tensorflow:loss = 49.220093, step = 500 (29.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.42117\n",
      "INFO:tensorflow:loss = 55.353493, step = 600 (29.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.38677\n",
      "INFO:tensorflow:loss = 51.25213, step = 700 (29.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.42546\n",
      "INFO:tensorflow:loss = 48.188995, step = 800 (29.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.41731\n",
      "INFO:tensorflow:loss = 47.146423, step = 900 (30.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.26815\n",
      "INFO:tensorflow:loss = 49.45704, step = 1000 (29.445 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into gs://bigus/colab_test_3/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "latent_size 2\n",
      "kernel (8, 64)\n",
      "stride (1, 2)\n",
      "final_conv [16, 32, 32]\n",
      "input Tensor(\"lqad_encoder/Reshape:0\", shape=(?, 16, 128, 1), dtype=float32)\n",
      "output Tensor(\"lqad_encoder/conv2d/Relu:0\", shape=(?, 16, 64, 16), dtype=float32)\n",
      "output Tensor(\"lqad_encoder/conv2d_1/Relu:0\", shape=(?, 16, 32, 32), dtype=float32)\n",
      "d_input Tensor(\"lqad_decoder/Reshape:0\", shape=(?, 16, 32, 32), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/conv2d_transpose/BiasAdd:0\", shape=(?, 16, 64, 16), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/conv2d_transpose_1/BiasAdd:0\", shape=(?, 16, 128, 1), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/Reshape_1:0\", shape=(?, 16, 128), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-03-01:54:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://bigus/colab_test_3/model.ckpt-1001\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-03-01:55:08\n",
      "INFO:tensorflow:Saving dict for global step 1001: distortion = 37.853397, divergence = 3.4732254, elbo = -41.326622, global_step = 1001, loss = 41.326622\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1001: gs://bigus/colab_test_3/model.ckpt-1001\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "latent_size 2\n",
      "kernel (8, 64)\n",
      "stride (1, 2)\n",
      "final_conv [16, 32, 32]\n",
      "input Tensor(\"lqad_encoder/Reshape:0\", shape=(?, 16, 128, 1), dtype=float32)\n",
      "output Tensor(\"lqad_encoder/conv2d/Relu:0\", shape=(?, 16, 64, 16), dtype=float32)\n",
      "output Tensor(\"lqad_encoder/conv2d_1/Relu:0\", shape=(?, 16, 32, 32), dtype=float32)\n",
      "d_input Tensor(\"lqad_decoder/Reshape:0\", shape=(?, 16, 32, 32), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/conv2d_transpose/BiasAdd:0\", shape=(?, 16, 64, 16), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/conv2d_transpose_1/BiasAdd:0\", shape=(?, 16, 128, 1), dtype=float32)\n",
      "d_output Tensor(\"lqad_decoder/Reshape_1:0\", shape=(?, 16, 128), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from gs://bigus/colab_test_3/model.ckpt-1001\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://bigus/colab_test_3/export/exporter/temp-b'1546480515'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 49.45704.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "ignored",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  FLAGS.data_dir = \"gs://bigus/data\"\n",
    "  FLAGS.model_dir = \"gs://bigus/colab_test_3\"\n",
    "  FLAGS.max_steps = 1001\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "vae_estimaotr_version_new.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
